"""
SQLite3 í”„ë¡œë•ì…˜ ìµœì í™”

WAL ëª¨ë“œ, PRAGMA ì„¤ì •, ì¸ë±ìŠ¤, ë°±ì—… ìë™í™”
"""
import sqlite3
import os
import logging
import shutil
from datetime import datetime
from typing import Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class SQLiteOptimizer:
    """
    SQLite3 ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ê´€ë¦¬ì
    """

    def __init__(self, db_path: str = "omni_db.sqlite"):
        """
        Args:
            db_path: SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
        """
        self.db_path = db_path
        self.backup_dir = Path("backups")
        self.backup_dir.mkdir(exist_ok=True)

    def optimize_for_production(self):
        """
        í”„ë¡œë•ì…˜ í™˜ê²½ì— ìµœì í™”ëœ PRAGMA ì„¤ì • ì ìš©
        """
        logger.info(f"Optimizing SQLite database: {self.db_path}")

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            # 1. WAL (Write-Ahead Logging) ëª¨ë“œ í™œì„±í™”
            # - ì½ê¸°ì™€ ì“°ê¸° ë™ì‹œ ìˆ˜í–‰ ê°€ëŠ¥
            # - ì„±ëŠ¥ í–¥ìƒ 30-50%
            cursor.execute("PRAGMA journal_mode=WAL")
            logger.info("âœ“ WAL mode enabled")

            # 2. Synchronous ëª¨ë“œ (NORMAL)
            # - FULL: ê°€ì¥ ì•ˆì „í•˜ì§€ë§Œ ëŠë¦¼
            # - NORMAL: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì•ˆì „í•˜ë©° ë¹ ë¦„
            # - OFF: ê°€ì¥ ë¹ ë¥´ì§€ë§Œ ìœ„í—˜ (ê¶Œì¥ ì•ˆ í•¨)
            cursor.execute("PRAGMA synchronous=NORMAL")
            logger.info("âœ“ Synchronous mode set to NORMAL")

            # 3. Cache Size (64MB)
            # - ê¸°ë³¸ê°’: 2MB (-2000 pages)
            # - ê¶Œì¥ê°’: 64MB (-64000 pages)
            cursor.execute("PRAGMA cache_size=-64000")
            logger.info("âœ“ Cache size set to 64MB")

            # 4. Temp Store (MEMORY)
            # - ì„ì‹œ í…Œì´ë¸”ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
            cursor.execute("PRAGMA temp_store=MEMORY")
            logger.info("âœ“ Temp store set to MEMORY")

            # 5. Memory-Mapped I/O (256MB)
            # - mmap ì‚¬ìš©ìœ¼ë¡œ ì½ê¸° ì„±ëŠ¥ í–¥ìƒ
            cursor.execute("PRAGMA mmap_size=268435456")  # 256MB
            logger.info("âœ“ Memory-mapped I/O enabled (256MB)")

            # 6. Auto Vacuum (INCREMENTAL)
            # - ê³µê°„ íšŒìˆ˜ë¥¼ ì ì§„ì ìœ¼ë¡œ ìˆ˜í–‰
            cursor.execute("PRAGMA auto_vacuum=INCREMENTAL")
            logger.info("âœ“ Auto vacuum set to INCREMENTAL")

            # 7. Page Size (4096 bytes)
            # - ê¸°ë³¸ê°’: 4096 (ëŒ€ë¶€ë¶„ì˜ ì‹œìŠ¤í…œì— ìµœì )
            cursor.execute("PRAGMA page_size=4096")
            logger.info("âœ“ Page size set to 4096 bytes")

            # 8. Busy Timeout (5ì´ˆ)
            # - ë°ì´í„°ë² ì´ìŠ¤ ì ê¸ˆ ì‹œ ëŒ€ê¸° ì‹œê°„
            cursor.execute("PRAGMA busy_timeout=5000")
            logger.info("âœ“ Busy timeout set to 5 seconds")

            conn.commit()
            logger.info("ğŸš€ SQLite optimization completed successfully")

        except Exception as e:
            logger.error(f"Failed to optimize SQLite: {e}")
            raise
        finally:
            conn.close()

    def create_indexes(self):
        """
        ìì£¼ ì¡°íšŒë˜ëŠ” ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ìƒì„±
        """
        logger.info("Creating database indexes...")

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        indexes = [
            # Campaigns
            "CREATE INDEX IF NOT EXISTS idx_campaigns_client ON campaigns(client_id)",
            "CREATE INDEX IF NOT EXISTS idx_campaigns_status ON campaigns(status)",
            "CREATE INDEX IF NOT EXISTS idx_campaigns_platform ON campaigns(platform)",

            # Contents
            "CREATE INDEX IF NOT EXISTS idx_contents_campaign ON contents(campaign_id)",
            "CREATE INDEX IF NOT EXISTS idx_contents_status ON contents(status)",
            "CREATE INDEX IF NOT EXISTS idx_contents_published ON contents(published_at)",

            # Script Blocks
            "CREATE INDEX IF NOT EXISTS idx_script_blocks_content ON script_blocks(content_id)",
            "CREATE INDEX IF NOT EXISTS idx_script_blocks_sequence ON script_blocks(sequence)",

            # Audio Generations
            "CREATE INDEX IF NOT EXISTS idx_audio_generations_task ON audio_generations(task_id)",
            "CREATE INDEX IF NOT EXISTS idx_audio_generations_content ON audio_generations(content_id)",
            "CREATE INDEX IF NOT EXISTS idx_audio_generations_status ON audio_generations(status)",

            # Performance Metrics
            "CREATE INDEX IF NOT EXISTS idx_performance_content ON performance_metrics(content_id)",
            "CREATE INDEX IF NOT EXISTS idx_performance_platform ON performance_metrics(platform)",
            "CREATE INDEX IF NOT EXISTS idx_performance_recorded ON performance_metrics(recorded_at)",

            # Audit Logs
            "CREATE INDEX IF NOT EXISTS idx_audit_logs_user ON audit_logs(user_id)",
            "CREATE INDEX IF NOT EXISTS idx_audit_logs_action ON audit_logs(action)",
            "CREATE INDEX IF NOT EXISTS idx_audit_logs_created ON audit_logs(created_at)",
        ]

        try:
            for index_sql in indexes:
                cursor.execute(index_sql)
                logger.info(f"âœ“ {index_sql.split('idx_')[1].split(' ')[0]}")

            conn.commit()
            logger.info("ğŸš€ All indexes created successfully")

        except Exception as e:
            logger.error(f"Failed to create indexes: {e}")
            raise
        finally:
            conn.close()

    def vacuum(self):
        """
        VACUUM ì‹¤í–‰ (ë°ì´í„°ë² ì´ìŠ¤ ì••ì¶• ë° ìµœì í™”)

        ì£¼ì˜: VACUUMì€ ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ë¥¼ ì¬ì‘ì„±í•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤.
        ê¶Œì¥: ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ (ì›” 1íšŒ)
        """
        logger.info("Running VACUUM (this may take a while)...")

        conn = sqlite3.connect(self.db_path)
        try:
            conn.execute("VACUUM")
            logger.info("âœ“ VACUUM completed")
        except Exception as e:
            logger.error(f"VACUUM failed: {e}")
            raise
        finally:
            conn.close()

    def analyze(self):
        """
        ANALYZE ì‹¤í–‰ (ì¿¼ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ í†µê³„ ìˆ˜ì§‘)

        ê¶Œì¥: ì¸ë±ìŠ¤ ìƒì„± í›„, ë°ì´í„° ëŒ€ëŸ‰ ë³€ê²½ í›„
        """
        logger.info("Running ANALYZE...")

        conn = sqlite3.connect(self.db_path)
        try:
            conn.execute("ANALYZE")
            logger.info("âœ“ ANALYZE completed")
        except Exception as e:
            logger.error(f"ANALYZE failed: {e}")
            raise
        finally:
            conn.close()

    def backup(self, backup_name: Optional[str] = None) -> str:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…

        Args:
            backup_name: ë°±ì—… íŒŒì¼ëª… (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ íƒ€ì„ìŠ¤íƒ¬í”„)

        Returns:
            ë°±ì—… íŒŒì¼ ê²½ë¡œ
        """
        if not backup_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"omni_db_backup_{timestamp}.sqlite"

        backup_path = self.backup_dir / backup_name

        logger.info(f"Creating backup: {backup_path}")

        try:
            # íŒŒì¼ ë³µì‚¬ (WAL ëª¨ë“œì—ì„œë„ ì•ˆì „)
            shutil.copy2(self.db_path, backup_path)

            # WAL íŒŒì¼ë„ ë°±ì—… (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
            wal_path = f"{self.db_path}-wal"
            if os.path.exists(wal_path):
                shutil.copy2(wal_path, f"{backup_path}-wal")

            # SHM íŒŒì¼ë„ ë°±ì—… (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
            shm_path = f"{self.db_path}-shm"
            if os.path.exists(shm_path):
                shutil.copy2(shm_path, f"{backup_path}-shm")

            logger.info(f"âœ“ Backup created: {backup_path}")
            return str(backup_path)

        except Exception as e:
            logger.error(f"Backup failed: {e}")
            raise

    def cleanup_old_backups(self, keep_count: int = 7):
        """
        ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì‚­ì œ

        Args:
            keep_count: ìœ ì§€í•  ë°±ì—… ê°œìˆ˜ (ê¸°ë³¸ 7ê°œ)
        """
        logger.info(f"Cleaning up old backups (keeping {keep_count})...")

        backups = sorted(
            self.backup_dir.glob("omni_db_backup_*.sqlite"),
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )

        deleted_count = 0
        for backup in backups[keep_count:]:
            try:
                backup.unlink()
                # WAL, SHM íŒŒì¼ë„ ì‚­ì œ
                wal_file = Path(f"{backup}-wal")
                shm_file = Path(f"{backup}-shm")
                if wal_file.exists():
                    wal_file.unlink()
                if shm_file.exists():
                    shm_file.unlink()

                deleted_count += 1
                logger.info(f"âœ“ Deleted old backup: {backup.name}")
            except Exception as e:
                logger.error(f"Failed to delete {backup}: {e}")

        logger.info(f"âœ“ Cleaned up {deleted_count} old backups")

    def get_database_info(self) -> dict:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ

        Returns:
            ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë° í†µê³„
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        info = {}

        try:
            # PRAGMA ì„¤ì • ì¡°íšŒ
            pragmas = [
                "journal_mode",
                "synchronous",
                "cache_size",
                "temp_store",
                "mmap_size",
                "auto_vacuum",
                "page_size",
                "page_count",
                "freelist_count"
            ]

            for pragma in pragmas:
                cursor.execute(f"PRAGMA {pragma}")
                value = cursor.fetchone()[0]
                info[pragma] = value

            # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°
            file_size_mb = os.path.getsize(self.db_path) / (1024 * 1024)
            info["file_size_mb"] = round(file_size_mb, 2)

            # í…Œì´ë¸” ê°œìˆ˜
            cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table'")
            info["table_count"] = cursor.fetchone()[0]

            # ì¸ë±ìŠ¤ ê°œìˆ˜
            cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='index'")
            info["index_count"] = cursor.fetchone()[0]

            return info

        finally:
            conn.close()


def optimize_database(db_path: str = "omni_db.sqlite"):
    """
    ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ ìµœì í™” (í”„ë¡œë•ì…˜ ë°°í¬ ì‹œ ì‹¤í–‰)

    Args:
        db_path: ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
    """
    optimizer = SQLiteOptimizer(db_path)

    # 1. ë°±ì—… ìƒì„±
    optimizer.backup()

    # 2. PRAGMA ìµœì í™”
    optimizer.optimize_for_production()

    # 3. ì¸ë±ìŠ¤ ìƒì„±
    optimizer.create_indexes()

    # 4. í†µê³„ ìˆ˜ì§‘
    optimizer.analyze()

    # 5. VACUUM (ì„ íƒì‚¬í•­ - ì‹œê°„ ì†Œìš”)
    # optimizer.vacuum()

    # 6. ì •ë³´ ì¶œë ¥
    info = optimizer.get_database_info()
    logger.info(f"Database Info: {info}")

    # 7. ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
    optimizer.cleanup_old_backups(keep_count=7)

    logger.info("âœ… Database optimization completed!")


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )

    # ìµœì í™” ì‹¤í–‰
    optimize_database()
